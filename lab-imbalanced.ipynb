{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB | Imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**\n",
    "\n",
    "In this challenge, we will be working with Credit Card Fraud dataset.\n",
    "\n",
    "https://raw.githubusercontent.com/data-bootcamp-v4/data/main/card_transdata.csv\n",
    "\n",
    "Metadata\n",
    "\n",
    "- **distance_from_home:** the distance from home where the transaction happened.\n",
    "- **distance_from_last_transaction:** the distance from last transaction happened.\n",
    "- **ratio_to_median_purchase_price:** Ratio of purchased price transaction to median purchase price.\n",
    "- **repeat_retailer:** Is the transaction happened from same retailer.\n",
    "- **used_chip:** Is the transaction through chip (credit card).\n",
    "- **used_pin_number:** Is the transaction happened by using PIN number.\n",
    "- **online_order:** Is the transaction an online order.\n",
    "- **fraud:** Is the transaction fraudulent. **0=legit** -  **1=fraud**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.877857</td>\n",
       "      <td>0.311140</td>\n",
       "      <td>1.945940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.829943</td>\n",
       "      <td>0.175592</td>\n",
       "      <td>1.294219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.091079</td>\n",
       "      <td>0.805153</td>\n",
       "      <td>0.427715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.247564</td>\n",
       "      <td>5.600044</td>\n",
       "      <td>0.362663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.190936</td>\n",
       "      <td>0.566486</td>\n",
       "      <td>2.222767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   distance_from_home  distance_from_last_transaction  \\\n",
       "0           57.877857                        0.311140   \n",
       "1           10.829943                        0.175592   \n",
       "2            5.091079                        0.805153   \n",
       "3            2.247564                        5.600044   \n",
       "4           44.190936                        0.566486   \n",
       "\n",
       "   ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "0                        1.945940              1.0        1.0   \n",
       "1                        1.294219              1.0        0.0   \n",
       "2                        0.427715              1.0        0.0   \n",
       "3                        0.362663              1.0        1.0   \n",
       "4                        2.222767              1.0        1.0   \n",
       "\n",
       "   used_pin_number  online_order  fraud  \n",
       "0              0.0           0.0    0.0  \n",
       "1              0.0           0.0    0.0  \n",
       "2              0.0           1.0    0.0  \n",
       "3              0.0           1.0    0.0  \n",
       "4              0.0           1.0    0.0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/card_transdata.csv\")\n",
    "fraud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        distance_from_home  distance_from_last_transaction  \\\n",
      "0                57.877857                        0.311140   \n",
      "1                10.829943                        0.175592   \n",
      "2                 5.091079                        0.805153   \n",
      "3                 2.247564                        5.600044   \n",
      "4                44.190936                        0.566486   \n",
      "...                    ...                             ...   \n",
      "999995            2.207101                        0.112651   \n",
      "999996           19.872726                        2.683904   \n",
      "999997            2.914857                        1.472687   \n",
      "999998            4.258729                        0.242023   \n",
      "999999           58.108125                        0.318110   \n",
      "\n",
      "        ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
      "0                             1.945940              1.0        1.0   \n",
      "1                             1.294219              1.0        0.0   \n",
      "2                             0.427715              1.0        0.0   \n",
      "3                             0.362663              1.0        1.0   \n",
      "4                             2.222767              1.0        1.0   \n",
      "...                                ...              ...        ...   \n",
      "999995                        1.626798              1.0        1.0   \n",
      "999996                        2.778303              1.0        1.0   \n",
      "999997                        0.218075              1.0        1.0   \n",
      "999998                        0.475822              1.0        0.0   \n",
      "999999                        0.386920              1.0        1.0   \n",
      "\n",
      "        used_pin_number  online_order  fraud  \n",
      "0                   0.0           0.0    0.0  \n",
      "1                   0.0           0.0    0.0  \n",
      "2                   0.0           1.0    0.0  \n",
      "3                   0.0           1.0    0.0  \n",
      "4                   0.0           1.0    0.0  \n",
      "...                 ...           ...    ...  \n",
      "999995              0.0           0.0    0.0  \n",
      "999996              0.0           0.0    0.0  \n",
      "999997              0.0           1.0    0.0  \n",
      "999998              0.0           1.0    0.0  \n",
      "999999              0.0           1.0    0.0  \n",
      "\n",
      "[1000000 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** What is the distribution of our target variable? Can we say we're dealing with an imbalanced dataset?\n",
    "- **2.** Train a LogisticRegression.\n",
    "- **3.** Evaluate your model. Take in consideration class importance, and evaluate it by selection the correct metric.\n",
    "- **4.** Run **Oversample** in order to balance our target variable and repeat the steps above, now with balanced data. Does it improve the performance of our model? \n",
    "- **5.** Now, run **Undersample** in order to balance our target variable and repeat the steps above (1-3), now with balanced data. Does it improve the performance of our model?\n",
    "- **6.** Finally, run **SMOTE** in order to balance our target variable and repeat the steps above (1-3), now with balanced data. Does it improve the performance of our model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "fraud\n",
      "0.0    912597\n",
      "1.0     87403\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage distribution:\n",
      "fraud\n",
      "0.0    91.26\n",
      "1.0     8.74\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. \n",
    "# Count occurrences of each class\n",
    "fraud_counts = fraud['fraud'].value_counts()\n",
    "fraud_percent = fraud['fraud'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display results\n",
    "print(\"Class distribution:\")\n",
    "print(fraud_counts)\n",
    "print(\"\\nPercentage distribution:\")\n",
    "print(fraud_percent.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class 0 (non-fraud): 91.26%\n",
    "\n",
    "- Class 1 (fraud): 8.74%\n",
    "\n",
    "this is an imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = fraud.drop(columns=['fraud'])\n",
    "y = fraud['fraud']\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "log_model = LogisticRegression(\n",
    "    solver='liblinear',       \n",
    "    class_weight='balanced',  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "log_model.fit(x_train, y_train)\n",
    "# Predict labels and probabilities\n",
    "y_pred = log_model.predict(x_test)\n",
    "y_proba = log_model.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9950    0.9330    0.9630    182557\n",
      "         1.0     0.5756    0.9513    0.7173     17443\n",
      "\n",
      "    accuracy                         0.9346    200000\n",
      "   macro avg     0.7853    0.9421    0.8401    200000\n",
      "weighted avg     0.9585    0.9346    0.9416    200000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[170325  12232]\n",
      " [   850  16593]]\n",
      "ROC-AUC Score: 0.9796\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(x_train, columns = x_train.columns)\n",
    "train[\"fraud\"] = y_train.values\n",
    "fraud = train[train[\"fraud\"] == 1]\n",
    "no_fraud = train[train[\"fraud\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9950    0.9332    0.9631    182557\n",
      "         1.0     0.5762    0.9512    0.7177     17443\n",
      "\n",
      "    accuracy                         0.9347    200000\n",
      "   macro avg     0.7856    0.9422    0.8404    200000\n",
      "weighted avg     0.9585    0.9347    0.9417    200000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[170354  12203]\n",
      " [   851  16592]]\n",
      "ROC-AUC Score: 0.9796\n"
     ]
    }
   ],
   "source": [
    "# 4 oversampling\n",
    "\n",
    "# Oversample the minority class\n",
    "fraud_oversampled = resample(\n",
    "    fraud,\n",
    "    replace=True,                # Sample with replacement\n",
    "    n_samples=len(no_fraud),   # Match majority class size\n",
    "    random_state=42              # For reproducibility\n",
    ")\n",
    "\n",
    "# Combine with majority class\n",
    "train_over = pd.concat([no_fraud, fraud_oversampled])\n",
    "\n",
    "\n",
    "X_train_over = train_over.drop(\"fraud\", axis=1)\n",
    "y_train_over = train_over[\"fraud\"]\n",
    "\n",
    "# Train model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "\n",
    "logreg.fit(X_train_over, y_train_over)\n",
    "\n",
    "# Predict\n",
    "y_pred = logreg.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, oversampling improved the model â€” not dramatically, but incrementally and safely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9951    0.9328    0.9630    182557\n",
      "         1.0     0.5752    0.9518    0.7171     17443\n",
      "\n",
      "    accuracy                         0.9345    200000\n",
      "   macro avg     0.7851    0.9423    0.8400    200000\n",
      "weighted avg     0.9585    0.9345    0.9415    200000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[170296  12261]\n",
      " [   841  16602]]\n",
      "ROC-AUC Score: 0.9796\n"
     ]
    }
   ],
   "source": [
    "# 5 undersampling\n",
    "\n",
    "train = pd.DataFrame(x_train, columns = x_train.columns)\n",
    "train[\"fraud\"] = y_train.values\n",
    "fraud = train[train[\"fraud\"] == 1]\n",
    "no_fraud = train[train[\"fraud\"] == 0]\n",
    "\n",
    "# Undersample the majority class\n",
    "no_fraud_undersampled = resample(\n",
    "    no_fraud,\n",
    "    replace=False,                 # No replacement\n",
    "    n_samples=len(fraud),        # Match minority class size\n",
    "    random_state=42                # For reproducibility\n",
    ")\n",
    "\n",
    "# Combine with minority class\n",
    "train_under = pd.concat([fraud, no_fraud_undersampled])\n",
    "\n",
    "X_train_under = train_under.drop(\"fraud\", axis=1)\n",
    "y_train_under = train_under[\"fraud\"]\n",
    "\n",
    "# Train model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "\n",
    "logreg.fit(X_train_under, y_train_under)\n",
    "\n",
    "# Predict\n",
    "y_pred = logreg.predict(x_test)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, undersampling did not improve the performance of the model, but it preserved it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9949    0.9337    0.9633    182557\n",
      "         1.0     0.5777    0.9500    0.7185     17443\n",
      "\n",
      "    accuracy                         0.9351    200000\n",
      "   macro avg     0.7863    0.9418    0.8409    200000\n",
      "weighted avg     0.9585    0.9351    0.9420    200000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[170446  12111]\n",
      " [   872  16571]]\n",
      "ROC-AUC Score: 0.9796\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "smote = SMOTE(random_state = 42,sampling_strategy=1.0)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "# Train model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "\n",
    "logreg.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict\n",
    "y_pred = logreg.predict(x_test)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, SMOTE improved the model, especially in terms of precision and F1-score for the minority class\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
